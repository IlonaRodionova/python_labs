import pytest
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "src"))

from lib.text import normalize, tokenize, count_freq, top_n


# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è normalize
@pytest.mark.parametrize(
    "source, expected",
    [
        ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
        ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
        ("Hello\r\nWorld", "hello world"),
        ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
        ("", ""),
        ("   ", ""),
        ("\n\t\r", ""),
        ("–¢–ï–ö–°–¢!! –°... punctuation?", "—Ç–µ–∫—Å—Ç!! —Å... punctuation?"),
    ],
)
def test_normalize(source, expected):
    assert normalize(source) == expected


# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è tokenize
@pytest.mark.parametrize(
    "text, expected",
    [
        ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
        ("hello world test", ["hello", "world", "test"]),
        ("", []),
        ("   ", []),
        ("–∑–Ω–∞–∫–∏, –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è! —Ç–µ—Å—Ç.", ["–∑–Ω–∞–∫–∏", "–ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è", "—Ç–µ—Å—Ç"]),
        ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
        ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
        ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
    ],
)
def test_tokenize(text, expected):
    assert tokenize(text) == expected


# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è count_freq
@pytest.mark.parametrize(
    "tokens, expected",
    [
        (
            ["apple", "banana", "apple", "cherry"],
            {"apple": 2, "banana": 1, "cherry": 1},
        ),
        ([], {}),
        (["hello"], {"hello": 1}),
        (["a", "a", "a", "b", "b"], {"a": 3, "b": 2}),
        (["test"], {"test": 1}),
    ],
)
def test_count_freq(tokens, expected):
    assert count_freq(tokens) == expected


# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è top_n
@pytest.mark.parametrize(
    "freq, n, expected",
    [
        ({"apple": 5, "banana": 3, "cherry": 7}, 2, [("cherry", 7), ("apple", 5)]),
        (
            {"apple": 3, "banana": 3, "cherry": 1},
            3,
            [("apple", 3), ("banana", 3), ("cherry", 1)],
        ),
        ({"apple": 2}, 5, [("apple", 2)]),
        ({}, 3, []),
        ({"a": 1, "b": 2, "c": 3}, 0, []),
        ({"x": 1, "y": 1, "z": 1}, 2, [("x", 1), ("y", 1)]),
    ],
)
def test_top_n(freq, n, expected):
    assert top_n(freq, n) == expected


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –∫–∞–∫ —É –æ–¥–Ω–æ–≥—Ä—É–ø–ø–Ω–∏–∫–∞
def test_normalize_special_cases():
    """–¢–µ—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
    assert normalize("  –ú–Ω–æ–≥–æ   –ø—Ä–æ–±–µ–ª–æ–≤  ") == "–º–Ω–æ–≥–æ –ø—Ä–æ–±–µ–ª–æ–≤"
    assert normalize("–†–∞–∑–Ω—ã–µ\n–ø–µ—Ä–µ–≤–æ–¥—ã\t—Å—Ç—Ä–æ–∫") == "—Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫"


def test_tokenize_edge_cases():
    """–¢–µ—Å—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
    assert tokenize("word") == ["word"]
    assert tokenize("a-b-c") == ["a-b-c"]
    assert tokenize("test1 test2 test3") == ["test1", "test2", "test3"]


def test_count_freq_edge_cases():
    """–¢–µ—Å—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç"""
    assert count_freq(["a", "A", "a"]) == {"a": 2, "A": 1}  # –†–µ–≥–∏—Å—Ç—Ä –≤–∞–∂–µ–Ω
    assert count_freq(["word-with-dash"]) == {"word-with-dash": 1}


def test_top_n_edge_cases():
    """–¢–µ—Å—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ top_n"""
    # –¢–µ—Å—Ç –∫–æ–≥–¥–∞ n –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    assert top_n({"a": 1}, 10) == [("a", 1)]
    # –¢–µ—Å—Ç —Å –æ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º
    assert top_n({"single": 5}, 1) == [("single", 5)]


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
def test_full_pipeline():
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º. –ú–∏—Ä –ø—Ä–µ–∫—Ä–∞—Å–µ–Ω."
    normalized = normalize(text)
    tokens = tokenize(normalized)
    freq = count_freq(tokens)
    top_words = top_n(freq, 2)

    assert normalized == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä! –ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º. –º–∏—Ä –ø—Ä–µ–∫—Ä–∞—Å–µ–Ω."
    assert tokens == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä", "–ø—Ä–∏–≤–µ—Ç", "–≤—Å–µ–º", "–º–∏—Ä", "–ø—Ä–µ–∫—Ä–∞—Å–µ–Ω"]
    assert freq == {"–ø—Ä–∏–≤–µ—Ç": 2, "–º–∏—Ä": 2, "–≤—Å–µ–º": 1, "–ø—Ä–µ–∫—Ä–∞—Å–µ–Ω": 1}
    assert top_words == [("–º–∏—Ä", 2), ("–ø—Ä–∏–≤–µ—Ç", 2)]


def test_full_pipeline_complex():
    """–ü–∞–π–ø–ª–∞–π–Ω —Å–æ —Å–ª–æ–∂–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º"""
    text = "–ü–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –ö–†–£–¢–û!!! 2025 –≥–æ–¥... –≤–µ—Ä—Å–∏—è 2.0"

    normalized = normalize(text)
    tokens = tokenize(normalized)
    freq = count_freq(tokens)
    top_words = top_n(freq, 3)

    assert "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É" in tokens
    assert "–∫—Ä—É—Ç–æ" in tokens
    assert "2025" in tokens
    assert "–≥–æ–¥" in tokens
    assert len(top_words) <= 3
